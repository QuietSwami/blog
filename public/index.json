[{"content":" Warning\nThis post is a draft and may not be accurate or complete. For the past few weeks I\u0026rsquo;ve been interested in exploring how data cooperatives can be extended by forming a Federation. And how these federations can be organized. Later, I\u0026rsquo;ll present in more detail what type of architecture I propose to implement this, but for now, let\u0026rsquo;s have a look on the idea of Federation of Data Cooperatives\nJust to give you a quick introduction, data cooperatives are groups of data producers under a common interest, who voluntarly pool their data under a common organization. This mainly benefits people or organizations who, otherwise, wouldn\u0026rsquo;t be able to sustain such a infrastructure on their own. The cooperative is created, owned, and managed by the community members, enabling the member to have full control on their data. Decision, for example, who can access and how the access is made, is taken by the members of the cooperative.\nWhile data cooperatives could be very helpful, the main disadvantege is the lack of scope. A data cooperative aggregates data from producers under a specific industry. Let\u0026rsquo;s take the case of agriculture. A data cooperative can be created by farmers that produce a specific crop, in order to pool their data together under a common goal. While this cooperative can be used by the farmers themselves, the data produced has little value besides to the farming operations. To extend the value of the data the farmers produce, data from other agricultural stakeholders might be required.\nA federation is a union of partially self-governing organizations under a common goal. We see federation in our daily life. We know that, for example, that states like Russia, United States, and Australia and others are federations, which means that the states that compose the country are autonumous to a certain degree. This degree depends on the country, but, in general, each state that composes the federation has a saying in the decisions of the country.\nExplain how governance is normally done In The Case of Data Explain how data can be federated, what are the goals, advantages and disadvantages Federating Learning (for Machine Learning) Explain ","permalink":"http://localhost:1313/posts/federation/","summary":"Warning\nThis post is a draft and may not be accurate or complete. For the past few weeks I\u0026rsquo;ve been interested in exploring how data cooperatives can be extended by forming a Federation. And how these federations can be organized. Later, I\u0026rsquo;ll present in more detail what type of architecture I propose to implement this, but for now, let\u0026rsquo;s have a look on the idea of Federation of Data Cooperatives","title":"Federation"},{"content":" Warning\nThis post is a draft and may not be accurate or complete. This note will focus on the reason why data cooperatives are needed (with a focus on agriculture), some architecture suggestions for their application, and why I think Dataspaces, as developed by IDSA might be a good solution for it.\n","permalink":"http://localhost:1313/posts/data_cooperatives/","summary":"Warning\nThis post is a draft and may not be accurate or complete. This note will focus on the reason why data cooperatives are needed (with a focus on agriculture), some architecture suggestions for their application, and why I think Dataspaces, as developed by IDSA might be a good solution for it.","title":"Data Cooperatives"},{"content":"Normally, there is no reason to publish drafts. I mean, who wants to display to the world their unfinished, unpolished and unverfied work? Aparently me. The idea was borned after I read Learning in Public by Shawn Wang. The premise is simple: while the majority of us learn in private, learning by publicaly displaying what you know about a subject, and submitting yourself to the wider public critisism has the ability of actually improving how deep you learn. Because there\u0026rsquo;s always someone out there who knows more than you.\nI decided to apply the same reason to my drafts. Once I started my PhD, I\u0026rsquo;ve noticed that I write supremely badly. Better yet, I tend to drive straight to the point, to a point only understandable by me. I lack a talent in explaining things, or at least explaining myself. After noticing that this would pose a problem later on in my PhD, I decided to create this blog, and share with the anyone who has any interest in any of the subjects I\u0026rsquo;ll discuss, what I\u0026rsquo;m doing trying to explain it in more detail. The second point is that I will use this website like a notebook, as a method to further extend my knowledge. Therefore, the idea to publish drafts is to invite (constructive) criticism from anyone who might know more than me, or from those who wish to learn about the subjects that I\u0026rsquo;m wrtting and aren\u0026rsquo;t undestanding much.\nSo, if you wish to form a (polite) complaint about any of my posts, please do not hesitate to write to my email\n","permalink":"http://localhost:1313/posts/publishingdrafts/","summary":"Normally, there is no reason to publish drafts. I mean, who wants to display to the world their unfinished, unpolished and unverfied work? Aparently me. The idea was borned after I read Learning in Public by Shawn Wang. The premise is simple: while the majority of us learn in private, learning by publicaly displaying what you know about a subject, and submitting yourself to the wider public critisism has the ability of actually improving how deep you learn.","title":"Publishing Drafts"},{"content":"Docker Swarm integrates clustering seamlessly with Docker, connecting Docker Daemons into a single, unified network. In a Docker Swarm cluster, one (or more) node is designated as the master (or manager), while the rest are worker nodes. The master node is responsible for distributing services across the worker nodes and ensuring that the desired state of each service is maintained.\nFor this demonstration, I have created a Docker Swarm cluster on AWS, consisting of 3 master nodes and 3 worker nodes. This setup is designed to test High Availability within Docker Swarm. By having multiple master nodes, the cluster remains operational even if one master node fails, ensuring continuous service availability.\nDNS-Based Discovery and Overlay Networks DNS-based service discovery is a fundamental feature of Docker Swarm that facilitates service-to-service communication. When you deploy a service in Docker Swarm, it automatically registers with the swarm\u0026rsquo;s internal DNS server. Each service is assigned a unique DNS name, which is accessible throughout the swarm cluster.\nThe internal DNS server in Docker Swarm manages the resolution of service names to their corresponding IP addresses. This allows containers to resolve service names to the IP addresses of the service\u0026rsquo;s tasks, enabling straightforward communication between services using DNS queries.\nDocker Swarm employs round-robin load balancing by default when multiple instances (tasks) of a service are running. When a service name is queried, the DNS server returns the IP addresses of all active tasks for that service. The Docker engine then balances incoming requests among these tasks, distributing the load evenly.\nEach service in a Docker Swarm can be accessed by its name. For example, if you deploy a service called web, other services can access it using the name web.\nExample First, we are going to create an Overlay network called my_network. This network will allow communication between containers deployed on different machines, enabling seamless service discovery and interaction across the swarm.\ndocker network create --driver overlay my_network --attachable Let\u0026rsquo;s inspect the network to verify its creation and configuration:\ndocker network inspect my_network [ { \u0026#34;Name\u0026#34;: \u0026#34;test_network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;j94uo2esqbrk0uy54c0xtq4dg\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2024-06-17T11:44:15.377226457Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;swarm\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;overlay\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;10.0.1.0/24\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;10.0.1.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: null, \u0026#34;Options\u0026#34;: { \u0026#34;com.docker.network.driver.overlay.vxlanid_list\u0026#34;: \u0026#34;4097\u0026#34; }, \u0026#34;Labels\u0026#34;: null } ] As you can see, there should be no peers nor any containers specified in the output. Peers are the nodes that are connected to the network, and they only connect once a service which uses the network is deployed on it.\nNext, we\u0026rsquo;ll deploy the first service, ensuring that we specify the network we want the service to be attached to:\ndocker service create --name web --replicas 3 --network my_network nginx Node: Docker automatically assigns DNS names for services. If you want to specify a custom DNS name, you can use the --dns-name flag. For example, --dns-name myweb will allow access via myweb.\nIn this example, we\u0026rsquo;ve created a service named web composed of 3 replicas running nginx. When this service is deployed, Docker Swarm automatically assigns DNS entries for it, allowing other services within the same network to resolve web to the IP addresses of its tasks.\nNow, to further test the DNS-Based Discovery, let\u0026rsquo;s run a busybox service, attach it to the same network, and then test the DNS resolution from within a container.\nNote: BusyBox is a lightweight, single executable that provides many common UNIX utilities, making it ideal for embedded systems and minimal environments. It combines tiny versions of many common commands, making it versatile and efficient for containerized environments.\ndocker service create --name app --replicas 3 --network my_network busybox:latest sleep 3600 Note: notice the sleep 3600. This will keep the busybox active for 1 hour. Without this command, the busybox starts and terminates immediately.\nWe can now check the status of the services to confirm they are running:\ndocker service ls Output:\nID NAME MODE REPLICAS IMAGE PORTS j94uo2esqbrk web replicated 3/3 nginx:latest k4t5u8w9x6y0 app replicated 3/3 busybox:latest To test the DNS resolution, we\u0026rsquo;ll access one of the busybox containers and attempt to ping the web service:\nIdentify a Running Container: First, SSH into a machine that is running the busybox container. You can find out which node is running the busybox containers by inspecting the tasks of the app service:\ndocker service ps app Access the container Once on the appropriate node, use docker exec to get a shell inside one of the busybox containers:\ndocker exec -it \u0026lt;busybox-container-id\u0026gt; /bin/sh or:\ndocker exec -it $(docker ps -q --filter \u0026#34;name=app\u0026#34;) /bin/sh Ping the web Service: Inside the container, use the ping command to test the DNS resolution of the web service:\nping web Output:\nPING web (10.0.1.2): 56 data bytes 64 bytes from 10.0.1.2: seq=0 ttl=64 time=0.045 ms 64 bytes from 10.0.1.3: seq=1 ttl=64 time=0.040 ms 64 bytes from 10.0.1.4: seq=2 ttl=64 time=0.042 ms This confirms that the web service is accessible by its DNS name web, and that the internal DNS server is correctly resolving the name to the IP addresses of the web service\u0026rsquo;s tasks. This DNS-based service discovery allows applications running inside the overlay network to communicate with each other seamlessly.\nIngress Load Balancing In Docker Swarm, the ingress load balancer distributes incoming network traffic across multiple service replicas, essential for scaling services and ensuring high availability.\nDocker Swarm uses an internal overlay network called the ingress network to handle incoming traffic. When a service is created with published ports, Docker Swarm automatically creates and manages the ingress network. This network spans all nodes in the swarm, enabling them to accept incoming traffic on the published ports.\nThe routing mesh is a critical component that routes incoming requests to any node in the Docker Swarm cluster, regardless of where the service tasks are running. The routing mesh ensures that traffic can reach the service even if its tasks are distributed across multiple nodes. This is achieved using IP Virtual Server (IPVS) for load balancing, which operates at the Linux kernel level to efficiently route traffic.\nPreviously, we\u0026rsquo;ve seen how to communicate between tasks in a swarm. Each node in the network keeps the DNS records of the overlay networks to which they belong. When a service is deployed, the internal DNS server updates all nodes with the necessary DNS records. This allows any node to resolve the service name to the appropriate virtual IP, ensuring that requests can be properly routed to available containers. The load balancing is performed in a round-robin fashion across healthy service instances.\nWhen a node receives a request for a published port, but the target container is not on that node, the routing mesh forwards the request to a node that is running the desired service instance. Since all nodes have a replica of the DNS records for the ingress network, they can efficiently redirect the traffic to the appropriate service instance.\n{{ $image := resources.Get \u0026ldquo;images/sunset.jpg\u0026rdquo; }}\nBenefits of Docker\u0026rsquo;s Ingress Load Balancer Simplifies Traffic Management: Automatically handles incoming traffic and distributes it across service instances. No need for external load balancers for basic load balancing needs. High Availability: Ensures that traffic is distributed evenly, preventing any single instance from becoming a bottleneck. Supports failover by routing traffic to healthy instances if some go down. Scalability: Easily scale services up or down, and the ingress load balancer adjusts traffic distribution accordingly. Flexibility: Supports various deployment modes and configurations to suit different use cases. Limitations Basic Load Balancing: The built-in load balancer uses a simple round-robin algorithm, which may not be sufficient for more complex traffic management needs. Performance Overhead: The routing mesh introduces some performance overhead due to the additional network hops. Limited Customization: For more advanced routing and load balancing features, external tools like Traefik or NGINX might be required. ","permalink":"http://localhost:1313/posts/mastering_docker_swarm_pt._1/","summary":"Docker Swarm integrates clustering seamlessly with Docker, connecting Docker Daemons into a single, unified network. In a Docker Swarm cluster, one (or more) node is designated as the master (or manager), while the rest are worker nodes. The master node is responsible for distributing services across the worker nodes and ensuring that the desired state of each service is maintained.\nFor this demonstration, I have created a Docker Swarm cluster on AWS, consisting of 3 master nodes and 3 worker nodes.","title":"Mastering Docker Swarm Pt.1"},{"content":"Python stands out as an exceptionally user-friendly programming language, offering straightforward fundamental concepts ideal for newcomers to coding. However, it also boasts several advanced features that enhance performance significantly. Among these features, Generators and Iterators are particularly noteworthy within the Python ecosystem. These tools are crucial for optimizing memory usage and implementing lazy evaluation, allowing programmers to handle large data sets more efficiently without the need for extensive memory allocation. Generators and Iterators enable the creation of efficient code that is not only resource-friendly but also cleaner and more readable, demonstrating Python’s versatility from basic scripting to handling complex, large-scale applications.\nAn iterator is an object that enables a programmer to traverse through all the elements of a collection, such as lists, tuples, dictionaries, and sets. This object adheres to the iterator protocol, which requires it to implement two specific methods: iter() and next(). The iter() method returns the iterator object itself and is used in conjunction with loops, like for and while, to repeatedly fetch the next element. The next() method, on the other hand, provides the next element of the collection and raises a StopIteration exception when there are no more elements to return. This mechanism underlies many of Python\u0026rsquo;s built-in functions and constructs, such as loops and comprehensions, allowing them to operate transparently over collections of data.\nGenerators are a type of iterable in Python that are used to generate a sequence of values. Unlike regular functions that return a single value and terminate, generators yield multiple values sequentially, pausing after each yield and resuming from where they left off. This is accomplished using the yield keyword. Generators are particularly useful for creating efficient, clean code, especially when dealing with large data sets or streams, as they provide values on demand and consume memory only when generating values. By maintaining state in local variables and control flow, generators facilitate complex computations without sacrificing readability or efficiency of the code. Essentially, they allow programmers to write a function that can send back a value and later resume to pick up where it left off.\nOverview of iter() and next() functions The iter() and next() functions are fundamental to the functionality of iterators in Python, facilitating the process of iterating over iterable objects such as lists, tuples, and dictionaries. Understanding how these functions operate is key to leveraging Python\u0026rsquo;s iteration capabilities more effectively.\niter() Function The iter() function is used to obtain an iterator from an iterable object. An iterable is any Python object capable of returning its members one at a time, permitting it to be iterated over in a loop. Common iterables include all types of collections, such as lists, tuples, and dictionaries. To make an object iterable, it typically needs to implement the __iter__() method, which returns an iterator object. This iterator object then processes the elements of the iterable one at a time.\nWhen iter(object) is called, Python internally calls object.__iter__() and returns an iterator for that object. If the object does not implement __iter__() but has a __getitem__() method, Python creates an iterator that attempts to fetch elements sequentially starting from index 0, raising an IndexError when no more elements are available.\nnext() Function Once an iterator is obtained using iter(), the next() function is used to sequentially access elements from the iterator. Calling next(iterator) internally invokes the iterator\u0026rsquo;s __next__() method, which returns the next available item from the iterable. If no more elements are available, it raises the StopIteration exception, signaling that the iteration is complete.\nThe use of next() is essential in low-level operations where control over the iteration process is required. This function allows developers to manually control the iteration, fetching the next element only when needed. This is particularly useful in scenarios where the iteration may need to be paused, altered, or conditionally advanced.\nPractical Usage Together, iter() and next() offer a powerful mechanism for custom iteration without the need for explicit loop constructs like for or while. This can lead to more expressive, efficient, and readable code, especially in situations involving advanced iteration patterns or when integrating with other Python features like generators and coroutines.\nBy using these functions, Python programmers can create highly efficient and lazy evaluations of potentially large datasets, manage streams of data in real-time, or implement custom iteration logic that goes beyond traditional loop constructs. Thus, understanding and using iter() and next() not only enriches one\u0026rsquo;s grasp of Python\u0026rsquo;s iteration model but also opens up possibilities for writing more pythonic and performance-oriented code.\nDeep Dive into Iterators One of the key features of iterators is their ability to maintain state. When you create an iterator, it internally keeps track of its position within the iterable. Each call to next() advances the position and returns the corresponding element. This statefulness allows iterators to handle large datasets efficiently, as they only process elements on demand and do not require the entire dataset to be loaded into memory.\nIterators are designed to be lazy, meaning they only compute and retrieve elements as they are needed. This on-demand way of fetching items makes iterators particularly useful for working with large datasets or data streams where you don\u0026rsquo;t want to consume resources for the entire data at once.\nCreating Custom Iterators Creating custom iterators can be very beneficial for handling specific types of data processing or implementing complex iteration logic. Here are some important use-cases for custom iterators in Python, with implementation examples for each.\n1. Iterating Over Infinite Sequences One common use-case is to generate infinite sequences, which can be particularly useful for generating an endless supply of data on-demand.\nclass InfiniteCounter: def __init__(self, start=0): self.current = start def __iter__(self): return self def __next__(self): current_value = self.current self.current += 1 return current_value # Usage: counter = InfiniteCounter(10) for num in counter: if num \u0026gt; 20: break print(num) This iterator starts counting from the specified number and increments indefinitely until the loop is manually stopped.\n2. Accessing Tree-like Data Structure Navigating complex data structures, like trees, can benefit from custom iterators that yield elements in a specified traversal order.\nclass TreeNode: def __init__(self, key): self.left = None self.right = None self.val = key class BinaryTree: def __init__(self, root): self.root = root def __iter__(self): return self.in_order_traversal(self.root) def in_order_traversal(self, node): if node: yield from self.in_order_traversal(node.left) yield node.val yield from self.in_order_traversal(node.right) # Usage: root = TreeNode(1) root.left = TreeNode(2) root.right = TreeNode(3) root.left.left = TreeNode(4) root.left.right = TreeNode(5) tree = BinaryTree(root) for value in tree: print(value) This example yields nodes\u0026rsquo; values in in-order (left-root-right) sequence.\n3. Reading Large Files Line by Line Reading large files efficiently without loading the entire file into memory can be achieved with a custom iterator.\nclass FileIterator: def __init__(self, filepath): self.filepath = filepath def __iter__(self): self.file = open(self.filepath, \u0026#39;r\u0026#39;) return self def __next__(self): line = self.file.readline() if line == \u0026#39;\u0026#39;: self.file.close() raise StopIteration return line.strip() # Usage: file_iter = FileIterator(\u0026#39;example.txt\u0026#39;) for line in file_iter: print(line) This iterator reads a file line by line, making it ideal for processing large text files.\n4. Cycling Through a Collection Cycling through a list or any collection repeatedly can be useful for simulations or games.\nclass Cycle: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if not self.data: raise StopIteration result = self.data[self.index % len(self.data)] self.index += 1 return result # Usage: cycler = Cycle([1, 2, 3]) for _ in range(10): print(next(cycler), end=\u0026#39; \u0026#39;) # Prints: 1 2 3 1 2 3 1 2 3 1 This iterator repeats the elements in the list indefinitely.\nThese examples showcase how custom iterators can be tailored to fit specific needs, enhancing the functionality and efficiency of Python applications.\nPython\u0026rsquo;s built-in iterators Another way to improve performance and readability of your Python code it\u0026rsquo;s the built-in interators. They offer a streamlined and efficient mechanism to traverse various data types. Here, we\u0026rsquo;ll explore some of the most commonly used built-in iterators and how they facilitate opereation on Python\u0026rsquo;s data structures.\nIterators from Collections Lists, Tuples, Strings, and Sets\nAll of these data types are iterable, meaning you can use a loop directly on them, which implicitly creates an iterator:\nmy_list = [1, 2, 3] for item in my_list: print(item) When you use a loop on these collections, Python internally calls iter() on the collection, which returns an iterator that goes through each element until it raises a StopIteration exception, signaling that there are no more elements.\nDictionaries\nKeys: By default, iterating over a dictionary iterates over its keys.\nmy_dict = {\u0026#39;a\u0026#39;: 1, \u0026#39;b\u0026#39;: 2} for key in my_dict: print(key) # Outputs \u0026#39;a\u0026#39; \u0026#39;b\u0026#39; Values: To iterate over the values, you use .values().\nfor value in my_dict.values(): print(value) # Outputs 1 2 Items: To iterate over the items as key-value pairs, you use .items().\nfor key, value in my_dict.items(): print(key, value) # Outputs \u0026#39;a\u0026#39; 1, \u0026#39;b\u0026#39; 2 File Iterators When you open a file in Python, you can iterate over its lines directly, which is much more memory-efficient than loading the entire file into a list:\nwith open(\u0026#39;example.txt\u0026#39;, \u0026#39;r\u0026#39;) as file: for line in file: print(line.strip()) This method is particularly useful for processing large files, as it reads one line at a time directly from the file stream.\nrange() Iterator The range() function returns an immutable sequence of numbers and is commonly used for looping a specific number of times in for loops:\nfor i in range(5): print(i) # Outputs 0 1 2 3 4 range() is especially efficient because it generates each number on the fly and does not store the entire range in memory.\nzip() Iterator zip() is used to iterate over several iterables in parallel, producing tuples containing elements from each iterable:\nnames = [\u0026#39;Alice\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;Charlie\u0026#39;] scores = [85, 90, 88] for name, score in zip(names, scores): print(f\u0026#34;{name}: {score}\u0026#34;) zip() stops when the shortest iterable is exhausted.\nenumerator() Iterator enumerate() adds a counter to an iterable and returns it in a form of enumerate object. This can be directly used in for loops to get index-value pairs:\nfor index, value in enumerate([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;]): print(f\u0026#34;{index}: {value}\u0026#34;) reversed() Iterator reversed() returns a reverse iterator that accesses the given sequence in the reverse order:\nfor char in reversed(\u0026#34;hello\u0026#34;): print(char) Understanding Generators Generators are a special type of iterator in Python that allow you to declare a function that behaves like an iterator, i.e., it can be used in a loop to return data one at a time. They provide a very powerful and versatile mechanism to handle sequences of data without needing to store them in memory all at once. Understanding how generators differ from regular functions and how they manage state can greatly enhance your ability to write efficient and effective Python code.\n1. Return Type and Behavior:\nNormal Function: A normal function processes its inputs and computes a result. Once a function executes the return statement, it completes its execution and returns control back to the caller along with any result, terminating the process.\nGenerator Function: Instead of returning a final result, a generator function yields a sequence of results over time, pausing after each yield and resuming from where it left off. This is done using the yield keyword instead of return. Unlike a regular function, which terminates after a return statement is executed, a generator function is designed to suspend and resume its execution and state around the last point of value generation.\n2. Memory Utilization:\nNormal Function: When invoked, a normal function must compute and return its entire result set at once, which can be a problem with large data sets due to memory constraints.\nGenerator Function: Generators yield items one at a time, only when required, thus occupying memory only for the item that is currently being processed. This makes them extremely memory efficient, particularly for large datasets and streams.\n3. Use Cases:\nNormal Function: Best used when you need to compute and use all results at once, or when the outputs are independent of the function\u0026rsquo;s state. Generator Function: Ideal for large data processing tasks, lazy evaluations, or when the complete data set does not need to be held in memory simultaneously. They are also great for pipelines where data flows through a series of processing steps.\nHow Generators Save State Generators in Python automatically maintain their states in the background, which allows them to resume where they left off when next() is called again. Here’s how they manage this:\n1. Local Variables and Execution State: Generators save their local variables and execution state between yields. When a generator yields, the state of the local variables and the point in the code execution is saved. The next time the generator\u0026rsquo;s next() method is invoked, it resumes right after the last yield run.\n2. Automatic State Management: The state management is handled automatically by Python, so developers do not need to write additional code to manage state, which simplifies the creation of iterators considerably and reduces the chance of bugs.\n3. Yield Keyword: The yield keyword is pivotal in a generator. When Python encounters yield, it sends the yielded value back to the caller but retains enough state to enable the function to be resumed right after the yield. This contrasts sharply with return, which exits a function entirely, cleaning up the local namespace.\n4. Generator Object: When a generator function is called, Python returns a generator object that can be iterated over, rather than executing the function. This object represents an ongoing generator process, with methods like next() to manually fetch the next value and send() to alter the flow of the generator.\nBy leveraging the ability of generators to yield multiple values sequentially and save their execution state between yields, Python programmers can handle data streams more efficiently and with greater control than is possible using only regular functions. This makes generators an essential tool in the Python programming toolkit, especially for applications involving large data processing or that require lazy execution.\nWriting Generator Functions This section explores how to write generator functions using the yield statement, provides examples of simple generator functions, and discusses the advantages of using generators, particularly focusing on memory efficiency and their capability to represent infinite sequences.\nThe yield statement is used in generator functions to specify that the function should return a value but suspend its state until it is called again. Unlike return, which exits a function entirely after executing, yield pauses the function, saving its state, and later resumes from where it left off. Here\u0026rsquo;s how to use it:\ndef generator_function(): yield value Each time the generator\u0026rsquo;s next() method is called, the function executes or resumes execution until it encounters a yield statement. After yielding, the function state is suspended, awaiting the next call.\nExamples of Simple Generator Functions This generator function counts indefinitely from a given number.\ndef count_from(start): while True: yield start start += 1 counter = count_from(10) print(next(counter)) # 10 print(next(counter)) # 11 Generates the Fibonacci sequence, where each number is the sum of the two preceding ones.\ndef fibonacci(): a, b = 0, 1 while True: yield a a, b = b, a + b fib = fibonacci() for _ in range(5): print(next(fib)) # Outputs: 0, 1, 1, 2, 3 Advantages of Generators\nMemory Efficiency Generators are highly memory-efficient. They yield one item at a time, only holding the last generated item in memory. This is particularly beneficial when working with large data sets, as it means not all data must be loaded into memory at once, unlike list comprehensions or storing entire data sets in lists.\nRepresenting Infinite Sequences Generators are ideal for representing infinite sequences. They allow the generation of data elements on-the-fly and do not require the data to fit into memory. This makes generators perfect for generating an unlimited series of values, such as infinite mathematical sequences or continuous data streams.\nNon-blocking Behaviour Generators can yield control back to the event loop, allowing other operations to run while waiting for an operation to complete. This is akin to non-blocking IO operations where the system can continue performing other tasks while waiting for data from a network request or file IO.\nEasier Error Handling and Resource Management In asynchronous programming, managing resources and errors can be complex due to the non-linear execution flow. Generators simplify these tasks by using yield in a context that naturally supports exception handling and cleanup actions.\nStreamlined Syntax for Asynchronous Code By using generator-based coroutines (prior to the introduction of async/await), Python allowed developers to write asynchronous code that looks and behaves like synchronous procedural code. This helps in reducing cognitive load and simplifying the codebase.\nHere’s an illustrative example, using Python\u0026rsquo;s older coroutine style with generators in an asynchronous task:\nimport time def asynchronous_task(): def task(): print(\u0026#34;Start task\u0026#34;) time.sleep(3) # Simulate a blocking operation print(\u0026#34;Finish task\u0026#34;) def after_task(): print(\u0026#34;Callback after task\u0026#34;) yield task() # Yield execution to run the task yield after_task() # Continue with the rest after yielding back # Simulate event loop processing gen = asynchronous_task() next(gen) # Execute task next(gen) # Execute after_task In this way, generators provide a foundational technique for managing asynchronous operations, showcasing their versatility beyond simple iteration and memory efficiency. This advantage remains highly relevant even as modern Python uses async and await for asynchronous programming, reflecting the principles first pioneered by generators.\nAdvanced Generator Features Generators in Python are not just simple iterators; they come equipped with advanced capabilities that can significantly enhance your programming, especially when dealing with complex workflows and data processing tasks. These advanced features include sending values to generators, using yield from for delegation, and building data pipelines. Let\u0026rsquo;s delve into each of these features:\nSending Values to Generators Generators can not only yield values but also receive information at the point of each yield, enabling a two-way exchange between the generator and its caller. This is accomplished using the send() method, which resumes the generator’s execution and simultaneously sends a value back to the generator. This value can be used inside the generator, typically to influence its behavior.\ndef ping_pong(): ball = yield \u0026#34;Ping\u0026#34; while True: if ball == \u0026#34;Ping\u0026#34;: ball = yield \u0026#34;Pong\u0026#34; else: ball = yield \u0026#34;Ping\u0026#34; # Create generator game = ping_pong() print(next(game)) # Start the game, prints \u0026#34;Ping\u0026#34; print(game.send(\u0026#34;Ping\u0026#34;)) # Send \u0026#34;Ping\u0026#34;, prints \u0026#34;Pong\u0026#34; print(game.send(\u0026#34;Pong\u0026#34;)) # Send \u0026#34;Pong\u0026#34;, prints \u0026#34;Ping\u0026#34; In this example, the ping_pong generator keeps alternating responses based on the value sent to it. The initial call to next() is required to start the generator.\nSimplifying Generator Delegation\ndef count_up_to(max): count = 1 while count \u0026lt;= max: yield count count += 1 def count_down_from(max): yield from count_up_to(max) while max \u0026gt; 0: yield max max -= 1 # Usage counter = count_down_from(3) for x in counter: print(x) # Outputs: 1, 2, 3, 3, 2, 1 This example demonstrates using yield from to delegate the counting up part to another generator, which the count_down_from function then extends to count back down. The yield from expression is used in Python generators to delegate part of its operation to another generator. This provides a way to compose generators together, allowing one generator to yield all values from another generator before continuing with its own execution.\nIn the count_down_from function, yield from is used to incorporate all the values from the count_up_to generator into the count_down_from generator. Here\u0026rsquo;s what happens step-by-step:\nExecution of count_up_to: When count_down_from reaches the line with yield from count_up_to(max), it starts executing the count_up_to generator. Yielding from count_up_to: The count_up_to generator begins to yield values from 1 up to the maximum value (max). Each value yielded by count_up_to is passed directly to the caller of count_down_from—as if count_down_from were yielding these values itself. Completion of count_up_to: Once count_up_to has yielded all its values and completes, control returns to count_down_from. Importantly, count_up_to needs to complete all its yields (i.e., it has to exhaust all its values by reaching its own end) before count_down_from continues beyond the yield from expression. Continuation of count_down_from: After count_up_to finishes, count_down_from resumes execution immediately after the yield from line. It then proceeds to yield its own values, counting down from max to 1. Generators can be effectively used to build data pipelines where each generator processes some data and passes it on to the next generator in the pipeline. This modular approach can lead to clean and manageable code, particularly suitable for data transformations and filtering.\ndef read_logs(file_path): \u0026#34;\u0026#34;\u0026#34;Read a log file line by line.\u0026#34;\u0026#34;\u0026#34; with open(file_path, \u0026#39;r\u0026#39;) as file: for line in file: yield line.strip() def filter_errors(log_lines): \u0026#34;\u0026#34;\u0026#34;Yield only error messages.\u0026#34;\u0026#34;\u0026#34; for line in log_lines: if \u0026#34;ERROR\u0026#34; in line: yield line def print_errors(errors): \u0026#34;\u0026#34;\u0026#34;Print each error line.\u0026#34;\u0026#34;\u0026#34; for error in errors: print(error) # Build the pipeline log_path = \u0026#39;server_logs.txt\u0026#39; log_lines = read_logs(log_path) error_lines = filter_errors(log_lines) print_errors(error_lines) This example illustrates how a data pipeline can be constructed using generators, with each function handling a specific part of the data processing, resulting in a clear and efficient workflow.\nPracticle Applications and Examples Generators are ideal for searching through data and applying filters without needing to load the entire dataset into memory. This can be particularly useful in scenarios where you are working with large amounts of data that are too big to fit into memory or when you want to apply complex filters progressively.\nSuppose you have a large log file and you need to find all entries that contain a specific error code. A generator can be used to yield only the lines that meet this criterion:\ndef filter_logs(filename, error_code): \u0026#34;\u0026#34;\u0026#34;Generator that yields log lines containing a specific error code.\u0026#34;\u0026#34;\u0026#34; with open(filename, \u0026#39;r\u0026#39;) as file: for line in file: if error_code in line: yield line.strip() # Usage error_entries = filter_logs(\u0026#39;system_logs.txt\u0026#39;, \u0026#39;Error 404\u0026#39;) for entry in error_entries: print(entry) In this example, the generator filter_logs reads the log file line by line, yielding only the lines that contain the specified error_code. This approach ensures that memory usage is minimal since only one line is held in memory at a time.\nReading large files is another practical application for generators, allowing for efficient processing of each line without the need for loading the entire file content into memory. This method is especially beneficial for handling large datasets, logs, or any large text files.\nImagine you have a large file containing financial transaction records, and you need to process each transaction to compute the total or apply some other analysis:\ndef read_transactions(file_path): \u0026#34;\u0026#34;\u0026#34;Generator that yields each transaction from a file.\u0026#34;\u0026#34;\u0026#34; with open(file_path, \u0026#39;r\u0026#39;) as file: next(file) # Skip the header line if there is one for line in file: data = line.strip().split(\u0026#39;,\u0026#39;) transaction = { \u0026#39;date\u0026#39;: data[0], \u0026#39;amount\u0026#39;: float(data[1]), \u0026#39;description\u0026#39;: data[2] } yield transaction # Usage total = 0 transactions = read_transactions(\u0026#39;transactions.csv\u0026#39;) for transaction in transactions: total += transaction[\u0026#39;amount\u0026#39;] print(f\u0026#34;Total amount of transactions: ${total:.2f}\u0026#34;) In this example, the generator read_transactions reads a CSV file containing transaction records. It processes each line individually to extract transaction details and yields a dictionary representing each transaction. This allows you to handle the processing of each record on-the-fly and compute the total amount incrementally.\nCommon Practices and Anti-Patterns Performance Considerations Conclusion ","permalink":"http://localhost:1313/posts/generators-and-iterators/","summary":"Python stands out as an exceptionally user-friendly programming language, offering straightforward fundamental concepts ideal for newcomers to coding. However, it also boasts several advanced features that enhance performance significantly. Among these features, Generators and Iterators are particularly noteworthy within the Python ecosystem. These tools are crucial for optimizing memory usage and implementing lazy evaluation, allowing programmers to handle large data sets more efficiently without the need for extensive memory allocation. Generators and Iterators enable the creation of efficient code that is not only resource-friendly but also cleaner and more readable, demonstrating Python’s versatility from basic scripting to handling complex, large-scale applications.","title":"Generators and Iterators"},{"content":"Python is renowned for its exceptional versatility, making it ideal for diverse applications, from web development to data science. In this blog post, we\u0026rsquo;re diving into a powerful feature known as Decorators. Although I had not explored Decorators for quite some time, I decided it was finally time to thoroughly understand and master them.\nUnderstanding Nested Functions in Python Before diving into decorators, it\u0026rsquo;s helpful to understand nested functions in Python. A nested function is defined within another function, and it can access variables from the enclosing scope. Here’s an example to illustrate this:\ndef outer(x): def inner(y): return x + y return inner adding = outer(9) # At this moment, the outer function is initialized with the argument 5. result = adding(10) # now we initilize the inner function with argument 6 print(result) # Result: 19 In the example above, inner is a nested function that adds a given number y to x, which is a parameter of the outer function outer. The nested function inner is only created and accessible within the outer function, not outside of it.\nNested functions are valuable for several reasons:\nEncapsulation: They help keep parts of code tightly coupled and hidden from the global scope. Scope Management: They can manipulate variables from their enclosing scope. Factory Functions: They can be used to create specific types of functions dynamically. Closures: They allow the inner function to remember the state of its environment, even after the outer function has finished executing. Decoding Decorators Moving from nested functions to decorators, a decorator in Python is essentially a function that takes another function and extends its behavior without permanently modifying it. This is a fundamental concept in Python, especially useful in scenarios like logging, access control, memoization, and more.\ndef my_decorator(func): def wrapper(): print(\u0026#34;Something happens before func execution\u0026#34;) func() print(\u0026#34;Something happens after func execution\u0026#34;) return wrapper @my_decorator def say_hello(): print(\u0026#34;Hello, World!\u0026#34;) Unlike Nested Functions, Decorators are specifically built to enhance or modify the behaviour of other functions. They provide a clear, readable way to apply common functionality to multiple functions or methods, while nested functions can only serve function described within.\nProperties Decorators have certain properties that should be noted. Understanding these allows us to extract more value from using decorators.\nStacking Decorators can be stacked, meaning one function can have more than one decorator. Each decorator wraps the function return by the decorator beneath it. This is particularly useful for combining functionalities such as logging, error handling and access control in a modular way\n@decorator2 @decorator1 def some_function(): return None When stacking decorators, the decorator closest to the function definition runs first as a wrapper, but last in execution flow around the function call itself. This is beacuse each decorator wraps the result of the previous decorator.\n# [ [ [ [ # F D F D G u e u e O o n c n c r A c o c o d T t r t r e i a i a r D o t o t i n o n o o a ] r ] r f g 1 1 r ] ] E a x m e [ [ c t D D u o e [ e [ t c D c D i I o e o e o l r c r c n l a o a o : u t r t r s o a o a f t r t r t u r 1 o 1 o n a ] r ] r c t 2 2 t e : ] : ] i o S f : E : n t u x a n d e E c c e c x k t c u e e i o t c d d o r e u e n a s t c D t e o e i o f s r c s r i a o 1 r n t r w s e o a r i t x r t a s t 1 o p r p w s e r d a i p d n b p e y e c P d o y d r t e b a h c y t o o o n r d r a e 2 t c o o r r 1 a t o r 2 Parameterization Decorators can be designed to take arguments. This requires the creation of a decorator factory, which returns a decorator. This is useful when you need to customize the behavior of the decorator for different functions:\ndef repeat(number_of_times): def decorator(func): def wrapper(*args, **kwargs): for _ in range(number_of_times): result = func(*args, **kwargs) return result return wrapper return decorator @repeat(number_of_times=3) def greet(name): print(f\u0026#34;Hello {name}\u0026#34;) greet(\u0026#34;Alice\u0026#34;) Metadata Preserverance By default decorators do not keep a function\u0026rsquo;s metadata (name, docstring and annotations). This is typically achieved using functools.wraps in the decorator\u0026rsquo;s wrapper function:\nimport functools def my_decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): \u0026#34;\u0026#34;\u0026#34;Wrapper function\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Something is happening before the function is called.\u0026#34;) return func(*args, **kwargs) return wrapper @my_decorator def say_hello(): \u0026#34;\u0026#34;\u0026#34;Say hello function\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Hello!\u0026#34;) print(say_hello.__name__) # Outputs \u0026#39;say_hello\u0026#39; print(say_hello.__doc__) # Outputs \u0026#39;Say hello function\u0026#39; Non-Functional Decorators (Class Decorators) Decorators aren\u0026rsquo;t limited to functions; they can also be used to be applied to classes. Class decorators can modify or enhance class behavior in a similar way to function decorators:\ndef decorator(cls): class WrappedClass(cls): def new_method(self): return \u0026#34;This is a new method added by the decorator\u0026#34; return WrappedClass @decorator class OriginalClass: def original_method(self): return \u0026#34;This method is part of the original class\u0026#34; obj = OriginalClass() print(obj.new_method()) # Outputs: This is a new method added by the decorator Usage Function Decorators Decorators should be used when there is several functions that need to share common functionality, avoiding code duplication. For example, a great use case is logging and debugging the execution of a program. Decorators can be used to log entry, exit and various states of function execution without cluttering the actual business logic with logging code.\ndef log_execution(func): def wrapper(*args, **kwargs): print(f\u0026#34;Entering {func.__name__}...\u0026#34;) result = func(*args, **kwargs) print(f\u0026#34;Exiting {func.__name__}...\u0026#34;) print(f\u0026#34;Execution status: {result}\u0026#34;) return result return wrapper @log_execution def my_function(): # function code here pass Decorators as Performance Measurment Another use case is the usage of Decorators as performance measurement. A decorator can be developed to add timing code that records how long a functions takes to run.\nimport time def measure_time(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() execution_time = end_time - start_time print(f\u0026#34;Function {func.__name__} took {execution_time} seconds to run.\u0026#34;) return result return wrapper @measure_time def my_function(): # function code here pass Decorators for Access Control Certain function will require a finer access control and authentication policies. A decorator can be used to add this to certain function, ensuring that only authorized users are able to execute ceratin functionalities. This is common in web frameworks where ceratin routes or actions are restricted to logged-in users or users with specific roles.\n# Simulating a user database and session users = { \u0026#39;alice\u0026#39;: \u0026#39;password123\u0026#39;, \u0026#39;bob\u0026#39;: \u0026#39;securepassword\u0026#39; } current_user = None def login(username, password): \u0026#34;\u0026#34;\u0026#34; Simulate user login, setting the current user if successful. \u0026#34;\u0026#34;\u0026#34; global current_user if username in users and users[username] == password: current_user = username print(f\u0026#34;{username} has logged in successfully.\u0026#34;) return True print(\u0026#34;Failed login attempt.\u0026#34;) return False def logout(): \u0026#34;\u0026#34;\u0026#34; Log out the current user \u0026#34;\u0026#34;\u0026#34; global current_user current_user = None print(\u0026#34;User has been logged out.\u0026#34;) def is_logged_in(): \u0026#34;\u0026#34;\u0026#34; Check if there\u0026#39;s a user logged in \u0026#34;\u0026#34;\u0026#34; return current_user is not None def login_required(func): \u0026#34;\u0026#34;\u0026#34; Decorator to ensure the user is logged in before calling the function \u0026#34;\u0026#34;\u0026#34; def wrapper(*args, **kwargs): if not is_logged_in(): raise Exception(\u0026#34;You must be logged in to access this function.\u0026#34;) return func(*args, **kwargs) return wrapper @login_required def protected_function(): \u0026#34;\u0026#34;\u0026#34; Function that only executes if the user is authenticated \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Access granted to {current_user}. Running protected function.\u0026#34;) # Example usage login(\u0026#39;alice\u0026#39;, \u0026#39;password123\u0026#39;) # Log in the user protected_function() # Should work as the user is logged in logout() # Log out the user try: protected_function() # Should raise an exception as the user is not logged in except Exception as e: print(e) Input Validation with Decorators Another useful use-case is the use of decorators as input validating, which can be used to check the arguments apssed to a function before being executed. This helps maintaining clean and error-free data processing within functions\ndef validate_inputs(func): \u0026#34;\u0026#34;\u0026#34; Decorator to validate that inputs to the function are positive numbers. \u0026#34;\u0026#34;\u0026#34; def wrapper(*args, **kwargs): # Check all positional arguments for arg in args: if not isinstance(arg, (int, float)) or arg \u0026lt;= 0: raise ValueError(\u0026#34;All dimensions must be positive numbers.\u0026#34;) # Check all keyword arguments for value in kwargs.values(): if not isinstance(value, (int, float)) or value \u0026lt;= 0: raise ValueError(\u0026#34;All dimensions must be positive numbers.\u0026#34;) return func(*args, **kwargs) return wrapper @validate_inputs def calculate_area(length, width): \u0026#34;\u0026#34;\u0026#34; Calculate the area of a rectangle given length and width, both must be positive numbers. \u0026#34;\u0026#34;\u0026#34; return length * width # Example usage try: print(\u0026#34;Area:\u0026#34;, calculate_area(5, 10)) # Valid input print(\u0026#34;Area:\u0026#34;, calculate_area(-5, 10)) # This should raise an exception except ValueError as e: print(e) try: print(\u0026#34;Area:\u0026#34;, calculate_area(length=5, width=3)) # Valid input print(\u0026#34;Area:\u0026#34;, calculate_area(length=5, width=-3)) # This should raise an exception except ValueError as e: print(e) Decorators as Caching Mechanisms Decorators can also be leverage as abastract caching mechanism. Any function that is decorated with this caching function can have their results cached before being returned. The following example displays how this caching mechanism can be used with a fibonnaci function. Before the decorated function is executed, the decorator retrieves any data that is cached and passes to the fibonnaci function.\ndef memorize(func): \u0026#34;\u0026#34;\u0026#34; Decorator that caches the results of the function calls. \u0026#34;\u0026#34;\u0026#34; cache = {} # Cache to store results of expensive function calls def wrapper(*args): if args in cache: return cache[args] # Return cached result if available result = func(*args) # Calculate result since it\u0026#39;s not cached cache[args] = result # Store the new result in cache return result return wrapper @memorize def fibonacci(n): \u0026#34;\u0026#34;\u0026#34; Recursive function to calculate the nth Fibonacci number. \u0026#34;\u0026#34;\u0026#34; if n in (0, 1): # Base cases return n return fibonacci(n-1) + fibonacci(n-2) # Recursive call # Example usage print(f\u0026#34;Fibonacci 10: {fibonacci(10)}\u0026#34;) # This should compute and then cache all values up to Fib(10) print(f\u0026#34;Fibonacci 20: {fibonacci(20)}\u0026#34;) # This should use cached values for Fib(0) to Fib(10) and compute the rest Enrinching Metadata with Decorators Decorators could be used to enrich metadata by appending additional information to the output of a function. The following example displays a nested decorator, which enriches the output of the decorated function.\ndef enrich_output(unit): \u0026#34;\u0026#34;\u0026#34; Decorator to enrich function outputs with additional data such as units. \u0026#34;\u0026#34;\u0026#34; def decorator(func): def wrapper(*args, **kwargs): result = func(*args, **kwargs) # Get the original function result enriched_result = { \u0026#39;value\u0026#39;: result, \u0026#39;units\u0026#39;: unit } # Enrich the output with additional information return enriched_result return wrapper return decorator @enrich_output(\u0026#39;square meters\u0026#39;) def calculate_area(length, width): \u0026#34;\u0026#34;\u0026#34; Calculate the area of a rectangle given length and width. \u0026#34;\u0026#34;\u0026#34; return length * width # Example usage area = calculate_area(5, 10) print(area) # Output will be a dictionary with the area and the units Event Handling with Decorators Flask uses decorators to handle events. When a request is done to a specific Flask endpoint, Flask treats the HTTP request and triggers an action described in the decorated function. Then, when the function finishes execution, it takes the output and transforms it into a HTTP response.\nimport functools import logging # Setting up logging logging.basicConfig(level=logging.INFO) def event_notifier(func): \u0026#34;\u0026#34;\u0026#34; Decorator to notify events when the function is called and after it completes. \u0026#34;\u0026#34;\u0026#34; @functools.wraps(func) def wrapper(*args, **kwargs): logging.info(f\u0026#34;Function {func.__name__} called with args: {args} and kwargs: {kwargs}\u0026#34;) try: result = func(*args, **kwargs) logging.info(f\u0026#34;Function {func.__name__} completed successfully with result: {result}\u0026#34;) return result except Exception as e: logging.error(f\u0026#34;Function {func.__name__} raised an error: {e}\u0026#34;) raise # Re-raise the exception after logging it return wrapper @event_notifier def add_numbers(a, b): \u0026#34;\u0026#34;\u0026#34; Adds two numbers and returns the result. \u0026#34;\u0026#34;\u0026#34; return a + b @event_notifier def fail_function(): \u0026#34;\u0026#34;\u0026#34; A function designed to fail to demonstrate error logging. \u0026#34;\u0026#34;\u0026#34; raise ValueError(\u0026#34;Intentional error for demonstration.\u0026#34;) # Example usage print(\u0026#34;Result of add_numbers:\u0026#34;, add_numbers(5, 10)) # Should log call and completion try: fail_function() # Should log the call and the error except ValueError as e: print(f\u0026#34;Caught an error: {e}\u0026#34;) Class Decorators As explained before, decorators aren\u0026rsquo;t restricted to functions, they can also be used to extend a class functionality. They can be used to extend the functionality of several classes with additional methods, or alter existing methods. This is commonly used in framewors or libraries where certain behaviors need to be standardized across multiple classes. Decorators should be used in lieu of inheritance where flexibility and reusability are needed, improving the seperation of concerns. They can be dynamically applyed, meaning, they are applied at runtime, depending on the conditions of the program\u0026rsquo;s execution or configuration.\ndef add_logging(cls): class WrappedClass(cls): def log_method(self, method_name): print(f\u0026#34;Method {method_name} started\u0026#34;) result = method_name() print(f\u0026#34;Method {method_name} ended\u0026#34;) return result def __getattr__(self, name): attr = getattr(super(), name) if callable(attr): def wrapper(*args, **kwargs): return self.log_method(lambda: attr(*args, **kwargs)) return wrapper return attr return WrappedClass @add_logging class SomeClass: def method_one(self): print(\u0026#34;Executing method one\u0026#34;) def method_two(self): print(\u0026#34;Executing method two\u0026#34;) obj = SomeClass() obj.method_one() ### Ensure Classes Adhere to Interface\nFurthermore, decorators can be used to ensure that certain classes adhere to a defined interface or abstract base class without using inheritance. This is particularly useful in large systems adhering to strict architectural patterns where specific methods must be implemented by multiple classes:\ndef ensure_interface(interface): def decorator(cls): missing_methods = [m for m in interface if not hasattr(cls, m)] if missing_methods: raise TypeError(f\u0026#34;Class {cls.__name__} does not implement {missing_methods}\u0026#34;) return cls return decorator @ensure_interface([\u0026#39;process\u0026#39;, \u0026#39;validate\u0026#39;]) class Processor: def process(self): pass # Implement the required method # Validate method is intentionally missing to show the error # This will raise a TypeError indicating the \u0026#39;validate\u0026#39; method is missing. Singletons Finally, decorators can be used to enforce the singleton pattern, limiting a class to a single instance throughout the lifetime of a program. This is commonly used in cases you need a controlled access point to a resrouce, such as a database connection or a configuration manager.\ndef singleton(cls): instances = {} def get_instance(*args, **kwargs): if cls not in instances: instances[cls] = cls(*args, **kwargs) return instances[cls] return get_instance @singleton class DatabaseConnection: def __init__(self): self.status = \u0026#39;Connected\u0026#39; # Both instances will actually be the same object db1 = DatabaseConnection() db2 = DatabaseConnection() print(db1 is db2) # Outputs: True ","permalink":"http://localhost:1313/posts/pythondecorators/","summary":"Python is renowned for its exceptional versatility, making it ideal for diverse applications, from web development to data science. In this blog post, we\u0026rsquo;re diving into a powerful feature known as Decorators. Although I had not explored Decorators for quite some time, I decided it was finally time to thoroughly understand and master them.\nUnderstanding Nested Functions in Python Before diving into decorators, it\u0026rsquo;s helpful to understand nested functions in Python.","title":"Decorators in Python"},{"content":"After one year in my PhD, I accumulated a lot of notes, both hand-written or written using tools like Obsidian, Notion, or similar. I\u0026rsquo;ve loss track of most notes I took, and do not actively keep track of most. I thought a blog was exactly what I needed, somewhere to discuss more in depth some of the work I\u0026rsquo;m developing, and other projects I come across.\nI hope this blog comes to serve some use to at least me - a place where I can develop further certain ideas, skills, and other tid bits I might find interesting.\nIf you want to follow what I\u0026rsquo;m working on, there always my GitHub.\n","permalink":"http://localhost:1313/posts/introduction/","summary":"After one year in my PhD, I accumulated a lot of notes, both hand-written or written using tools like Obsidian, Notion, or similar. I\u0026rsquo;ve loss track of most notes I took, and do not actively keep track of most. I thought a blog was exactly what I needed, somewhere to discuss more in depth some of the work I\u0026rsquo;m developing, and other projects I come across.\nI hope this blog comes to serve some use to at least me - a place where I can develop further certain ideas, skills, and other tid bits I might find interesting.","title":"Introduction"}]